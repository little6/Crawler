1.  你先从简单的写起，随便选一个网站作为目标，然后模拟http请求，httpclient可以，然后把爬下来的东西存起来，
思路就是这个思路，但是可以慢慢扩展.


2. 比方说一开始用单线程爬，后面用多线程爬，再后面可以用集群爬(集群维护同一个数据库)。


3. 数据存储，一开始可以用文本存，后续可以用数据库，如果你爬的网站有明确的格式，那可以在爬的时候就做解析，
然后放到MySQL之类的关系数据库里，要是没有明确的格式，可以放到mongodb里，后续再处理


4.  然后网址去重，就是已经爬过的内容不能再爬，你可以用数据库来帮你去重，也可以用布隆过滤器之类的东西来做


5.  还有反爬虫的东西，这个也很多，你可以慢慢做，慢慢加功能，最后对爬下来的数据做个分析，就是一个完整的系统了，
开源到github上。


6. 百万级别uri 可以用数据库去重，加上索引，可以持久化。 再提升性能可以考虑 布隆过滤器+redis,速度应该远高于数据库
	也可以考虑mongdb或则hbase 这些天生支持分布式。
	
	
7. 后面研究一下 nutch分布式爬虫







